{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DWM NAIVE BAYES AND SVM .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmir38YkVSF9"
      },
      "source": [
        "# NAIVE BAYES AND SVM IMPLEMENTATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Hd_s2YzVYOA"
      },
      "source": [
        "REG NO: 20MAI0015"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOzjdfi1ZDTZ"
      },
      "source": [
        "DATASET : https://www.kaggle.com/kazanova/sentiment140/download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKubQUOzAAUB"
      },
      "source": [
        "import numpy as numpy\n",
        "import pandas as pd"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtCDY-g4Jmb9"
      },
      "source": [
        "To visualize the data in the project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkwWUHIEDYU2"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h6mTzKLDZzb"
      },
      "source": [
        "import sklearn\n",
        "from sklearn.utils import shuffle \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s_Cv6oNJkdo"
      },
      "source": [
        "NLP Preprocessing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ISPGdrXDbK-"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4ZyRdK0DgCS"
      },
      "source": [
        "import re\n",
        "import random\n",
        "import gensim"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4NnHxbjDgWs"
      },
      "source": [
        "\n",
        "from collections import Counter\n",
        "import unicodedata as udata\n",
        "import string"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tos8BVInDhbg",
        "outputId": "39707541-ae81-4c6e-c77d-b7a0032a8c84"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxS_zbz2HYn-"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/My Drive/tw_data.csv\", encoding='latin-1', header=None)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL6ZpNNvJga7"
      },
      "source": [
        "\n",
        "Reading csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0u1UKx9HkoU"
      },
      "source": [
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtiAwA0OJpaX"
      },
      "source": [
        "Give column names - \n",
        "Assigning the Columns name to the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt5llyW2JarE"
      },
      "source": [
        "df.columns = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWXS5fJEJs5c",
        "outputId": "0be71b96-c0c3-416a-f136-4f236755f41d"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['sentiment', 'id', 'date', 'query', 'user', 'text'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJQeROFcJ00n"
      },
      "source": [
        "Checking Null values in the dataset. Here we are counting each cloumn null values in the data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFJBPhjJJwmb",
        "outputId": "52524115-07c6-4b03-cc7a-a91fc6668331"
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sentiment    0\n",
              "id           0\n",
              "date         0\n",
              "query        0\n",
              "user         0\n",
              "text         0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-as-x9tJ58G"
      },
      "source": [
        "Checking the duplicates values and counting duplicates in the data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgM1mOzRKCCK",
        "outputId": "52ac293f-794f-47bc-f93c-13e43483daf7"
      },
      "source": [
        "df.duplicated().sum()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "igTetC_AJ5qJ",
        "outputId": "910ce269-512c-4ccb-b4c4-c13b1d343d7e"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>query</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>1692567791</td>\n",
              "      <td>Sun May 03 20:16:45 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>StacyCBaker</td>\n",
              "      <td>@kalyanp Well, she's cute so I'll take the com...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1998046211</td>\n",
              "      <td>Mon Jun 01 17:54:07 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>allison_nikol</td>\n",
              "      <td>My fingers are cold</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2198044619</td>\n",
              "      <td>Tue Jun 16 15:31:58 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>KaioChilango</td>\n",
              "      <td>En el Punch Out ya me ganÃ³ el Rey Hippo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>2267094074</td>\n",
              "      <td>Sun Jun 21 09:36:55 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>felicityfuller</td>\n",
              "      <td>@FlissTee i am fine thanks - had a lovely week...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1981392999</td>\n",
              "      <td>Sun May 31 09:03:29 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>annataylor44</td>\n",
              "      <td>nothin getting readyy to go to my cuzins gradu...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment  ...                                               text\n",
              "0          4  ...  @kalyanp Well, she's cute so I'll take the com...\n",
              "1          0  ...                               My fingers are cold \n",
              "2          0  ...          En el Punch Out ya me ganÃ³ el Rey Hippo \n",
              "3          0  ...  @FlissTee i am fine thanks - had a lovely week...\n",
              "4          4  ...  nothin getting readyy to go to my cuzins gradu...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZY6lIUWJyLt"
      },
      "source": [
        "df = df.drop([\"id\", \"date\", \"query\", \"user\"], axis = 1)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "mYNhkQQsKGJ_",
        "outputId": "0cded13d-b0fa-4cc4-e66d-4bb0aa400416"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>@kalyanp Well, she's cute so I'll take the com...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>My fingers are cold</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>En el Punch Out ya me ganÃ³ el Rey Hippo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>@FlissTee i am fine thanks - had a lovely week...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>nothin getting readyy to go to my cuzins gradu...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment                                               text\n",
              "0          4  @kalyanp Well, she's cute so I'll take the com...\n",
              "1          0                               My fingers are cold \n",
              "2          0          En el Punch Out ya me ganÃ³ el Rey Hippo \n",
              "3          0  @FlissTee i am fine thanks - had a lovely week...\n",
              "4          4  nothin getting readyy to go to my cuzins gradu..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ortD9PSYKHyB",
        "outputId": "1b40b122-3272-4dda-f465-b38d2a1d4217"
      },
      "source": [
        "df.sentiment.value_counts()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4    800000\n",
              "0    800000\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-Bco3dAKOUK"
      },
      "source": [
        "Cleaning data -- \n",
        "add new column pre_clean_len to dataframe which is length of each tweet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElvaB4XeKJDe"
      },
      "source": [
        "df['pre_clean_len'] = [len(t) for t in df.text]\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpG_vgt4KUbM"
      },
      "source": [
        "Finding outliers using Box plot using pre_clean_len column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "sxNdfKZ6KRgb",
        "outputId": "a9569c51-549c-4b86-c1a2-c7d90f009935"
      },
      "source": [
        "plt.boxplot(df.pre_clean_len)\n",
        "plt.show()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSQlrvrbKaz3"
      },
      "source": [
        "\n",
        "check for any tweets greater than 140 characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "VmO5UGmBKYDR",
        "outputId": "41d9f243-9089-449b-e350-6c62a8c4099c"
      },
      "source": [
        "df[df.pre_clean_len > 140].head(10)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "      <th>pre_clean_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>0</td>\n",
              "      <td>AWESOME romantic getaway this weekend! woohoo!...</td>\n",
              "      <td>141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>213</th>\n",
              "      <td>0</td>\n",
              "      <td>It is an cycle  Im happy &amp;amp; ready for bed, ...</td>\n",
              "      <td>141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>493</th>\n",
              "      <td>4</td>\n",
              "      <td>@glitzyorbit Some suppliers wanted to take us ...</td>\n",
              "      <td>145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>533</th>\n",
              "      <td>4</td>\n",
              "      <td>@robromoni you said &amp;quot;you complete me!&amp;quo...</td>\n",
              "      <td>157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>552</th>\n",
              "      <td>4</td>\n",
              "      <td>I used the word cuss for lack of a better term...</td>\n",
              "      <td>146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>580</th>\n",
              "      <td>0</td>\n",
              "      <td>3rd &amp;amp; Pike smells like banana runts.  Also...</td>\n",
              "      <td>141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>627</th>\n",
              "      <td>4</td>\n",
              "      <td>Oh shout out 2 Justin &amp;amp; Nick who waited @ ...</td>\n",
              "      <td>146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>800</th>\n",
              "      <td>0</td>\n",
              "      <td>@Canageek ...profile page and hit &amp;quot;older&amp;...</td>\n",
              "      <td>145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>802</th>\n",
              "      <td>4</td>\n",
              "      <td>@optimiced (Ð¼Ð¾Ñ?Ñ twti Ð¸Ð¼Ð¿Ð»Ð¸ÑÐ¸ÑÐ½Ð¾...</td>\n",
              "      <td>163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>835</th>\n",
              "      <td>0</td>\n",
              "      <td>@RyanSeacrest Figures  how does a Canadian get...</td>\n",
              "      <td>146</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentiment  ... pre_clean_len\n",
              "82           0  ...           141\n",
              "213          0  ...           141\n",
              "493          4  ...           145\n",
              "533          4  ...           157\n",
              "552          4  ...           146\n",
              "580          0  ...           141\n",
              "627          4  ...           146\n",
              "800          0  ...           145\n",
              "802          4  ...           163\n",
              "835          0  ...           146\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHx6YJEdKecn"
      },
      "source": [
        "**removing outlier tweets**\n",
        "\n",
        "Cleaning operations\n",
        "\n",
        "Importing beautiful \n",
        "\n",
        "remove @ mentions from tweets\n",
        "\n",
        "remove URLs from tweets\n",
        "\n",
        "converting words like isn't to is not\n",
        "\n",
        "get only text from the tweets\n",
        "\n",
        "remove utf-8-sig code\n",
        "\n",
        "converting all into lower case\n",
        "\n",
        "will replace non-alphabetic characters by space\n",
        "\n",
        "Word Punct Tokenize and only consider \n",
        "\n",
        "words whose length is greater than 1\n",
        "\n",
        "join the words\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN-rnjdzKZ0Y"
      },
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "tok = WordPunctTokenizer()"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieS6AopRQ7k8"
      },
      "source": [
        "pat1 = r'@[A-Za-z0-9_]+'        # remove @ mentions from tweets\n",
        "pat2 = r'https?://[^ ]+'        # remove URLs from tweets\n",
        "combined_pat = r'|'.join((pat1, pat2)) #addition of pat1 and pat2\n",
        "www_pat = r'www.[^ ]+'         # remove URLs from tweets\n",
        "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",   # converting words like isn't to is not\n",
        "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
        "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
        "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
        "                \"mustn't\":\"must not\"}\n",
        "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ1hQ_P-Q9_r"
      },
      "source": [
        "def tweet_cleaner(text):  # define tweet_cleaner function to clean the tweets\n",
        "    soup = BeautifulSoup(text, 'lxml')    # create beautiful soup object\n",
        "    souped = soup.get_text()   # get only text from the tweets \n",
        "    try:\n",
        "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")    # remove utf-8-sig code\n",
        "    except:\n",
        "        bom_removed = souped\n",
        "    stripped = re.sub(combined_pat, '', bom_removed) # calling combined_pat\n",
        "    stripped = re.sub(www_pat, '', stripped) #remove URLs\n",
        "    lower_case = stripped.lower()      # converting all into lower case\n",
        "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case) # converting words like isn't to is not\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)       # will replace # by space\n",
        "    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1] # Word Punct Tokenize and only consider words whose length is greater than 1\n",
        "    return (\" \".join(words)).strip() # join the words"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvJgsoyqQ_3T",
        "outputId": "8f6d6e87-93b2-4c1c-ee65-01bcd9b8e551"
      },
      "source": [
        "#Note that we have 1600000 instances. But processing so many instances will take a very very long time.\n",
        "#Hence, restricting to rather 50000 instances.\n",
        "limit=50000\n",
        "import time; \n",
        "ms = time.time()\n",
        "#nums = [0,400000,800000,1200000,1600000] # used for batch processing tweets\n",
        "#nums = [0, 9999]\n",
        "clean_tweet_texts = [] # initialize list\n",
        "for i in range(0,limit): # batch process 1.6 million tweets \n",
        "    if i % 10000==0:\n",
        "        print(i, time.time()-ms)\n",
        "    clean_tweet_texts.append(tweet_cleaner(df['text'][i]))  # call tweet_cleaner function and pass parameter as all the tweets to clean the tweets and append cleaned tweets into clean_tweet_texts list"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.0001304149627685547\n",
            "10000 2.705563545227051\n",
            "20000 5.360196590423584\n",
            "30000 7.989318132400513\n",
            "40000 10.646017789840698\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NjgVr_aSNM6"
      },
      "source": [
        "clean tweet texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lp7IkcxzSMsV",
        "outputId": "7693c209-b695-4bab-fa31-a0e21a253c72"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN_UWk6mSTM-"
      },
      "source": [
        "tokenize word in clean_tweet_texts and append it to word_tokens list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agLZGb2-RCab"
      },
      "source": [
        "word_tokens = [] # initialize list for tokens\n",
        "for word in clean_tweet_texts:  # for each word in clean_tweet_texts\n",
        "    word_tokens.append(word_tokenize(word)) #tokenize word in clean_tweet_texts and append it to word_tokens list"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0M0oiJ7SYi4"
      },
      "source": [
        "\n",
        "Lemmatizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzOsIm4rSVLy",
        "outputId": "aebc8263-de2c-4347-af93-e55282f5694c"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngK9RhY-SaUK"
      },
      "source": [
        "df1 = [] # initialize list df1 to store words after lemmatization\n",
        "from nltk.stem import WordNetLemmatizer # import WordNetLemmatizer from nltk.stem\n",
        "lemmatizer = WordNetLemmatizer() # create an object of WordNetLemmatizer\n",
        "for l in word_tokens: # for loop for every tokens in word_token\n",
        "    b = [lemmatizer.lemmatize(q) for q in l] #for every tokens in word_token lemmatize word and giev it to b\n",
        "    df1.append(b) #append b to list df1"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDTbpbpfScZ4"
      },
      "source": [
        "clean_df1 =[] # initialize list clean_df1 to join word tokens after lemmatization\n",
        "for c in df1:  # for loop for each list in df1\n",
        "    a = \" \".join(c) # join words in list with space in between and give it to a\n",
        "    clean_df1.append(a) # append a to clean_df1"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBTtGq97Sh9k"
      },
      "source": [
        "Cleaning df_1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVDLWzrkSfMu"
      },
      "source": [
        "clean_df = pd.DataFrame(clean_df1,columns=['text']) # convert clean_tweet_texts into dataframe and name it as clean_df\n",
        "#clean_df['target'] = df.sentiment[:10000] # from earlier dataframe get the sentiments of each tweet and make a new column in clean_df as target and give it all the sentiment score\n",
        "#clean_df"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1akU559SmOo"
      },
      "source": [
        "clean_df['clean_len'] = [len(t) for t in clean_df.text] # Again make a new coloumn in the dataframe and name it as clean_len which"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "X-NrF3orSnpC",
        "outputId": "8a220a63-b554-4b6c-99d8-30f82d816998"
      },
      "source": [
        "target2 = [] # initialize list\n",
        "for i in range(0,limit): # batch process 1.6 million tweets \n",
        "    target2.append(df['sentiment'][i])\n",
        "clean_df['target']=target2\n",
        "df.head()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "      <th>pre_clean_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>@kalyanp Well, she's cute so I'll take the com...</td>\n",
              "      <td>109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>My fingers are cold</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>En el Punch Out ya me ganÃ³ el Rey Hippo</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>@FlissTee i am fine thanks - had a lovely week...</td>\n",
              "      <td>138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>nothin getting readyy to go to my cuzins gradu...</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment                                               text  pre_clean_len\n",
              "0          4  @kalyanp Well, she's cute so I'll take the com...            109\n",
              "1          0                               My fingers are cold              20\n",
              "2          0          En el Punch Out ya me ganÃ³ el Rey Hippo              41\n",
              "3          0  @FlissTee i am fine thanks - had a lovely week...            138\n",
              "4          4  nothin getting readyy to go to my cuzins gradu...             60"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k70Iv5_zStHC",
        "outputId": "1b54ab24-1e76-4e37-d320-5ad9355f6238"
      },
      "source": [
        "X = clean_df.text # get all the text in x variable\n",
        "y = clean_df.target # get all the sentiments into y variable\n",
        "print(X.shape) #print shape of x\n",
        "print(y.shape) # print shape of y\n",
        "from collections import Counter\n",
        "print(set(y)) # equals to list(set(words))\n",
        "print(Counter(y).values())"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000,)\n",
            "(50000,)\n",
            "{0, 4}\n",
            "dict_values([25188, 24812])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ceq3k_cASxiy"
      },
      "source": [
        "Perform train and test split\n",
        "\n",
        "X_train is the tweets of training data, X_test is the testing tweets which we have to predict, y_train is the sentiments of tweets in the traing data and y_test is the sentiments of the tweets which we will use to measure the accuracy of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHLw1xhXSvs1"
      },
      "source": [
        "from sklearn.model_selection  import train_test_split #from sklearn.cross_validation import train_test_split to split the data into training and tesing set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state= 1) # split the data into traing and testing set where ratio is 80:20"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHYyyvNKS9Lf"
      },
      "source": [
        "\n",
        "Get Tf-idf object and save it as vect. We can select features from here we just have simply change\n",
        "the ngram range to change the features also we can remove stop words over here with the help of stop parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeIYbRDDS2N2"
      },
      "source": [
        "vect = TfidfVectorizer(analyzer = \"word\", ngram_range=(1,3))"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMqw8CCETDfS"
      },
      "source": [
        "fit or training data tweets to vect"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzFL6DO3TAty"
      },
      "source": [
        "vect.fit(X_train) \n",
        "X_train_dtm = vect.transform(X_train)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76hDjG9_TF4v"
      },
      "source": [
        "X_test_dtm = vect.transform(X_test)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skZLCBLlTLt6"
      },
      "source": [
        "## **Naive Bayes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNzYPy0bTHli"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB # import Multinomial Naive Bayes model from sklearn.naive_bayes\n",
        "nb = MultinomialNB(alpha = 10) # get object of Multinomial naive bayes model with alpha parameter = 10"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmnFAMs6TUu8",
        "outputId": "43427ba9-e279-4ed7-b537-23ebd32c8969"
      },
      "source": [
        "nb.fit(X_train_dtm, y_train)# fit our both training data tweets as well as their sentiments to the multinomial naive bayes model"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=10, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRnipaFvTW9b",
        "outputId": "9cfba9d4-e96d-4d85-9b6f-6b169c61b2b5"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score  # import cross_val_score from sklear.model_selection\n",
        "accuracies = cross_val_score(estimator = nb, X = X_train_dtm, y = y_train, cv = 10) # do K- fold cross validation on our traing data and its sentimenst with 10 fold cross validation\n",
        "accuracies.mean() # measure the mean accuray of 10 fold cross validation"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.772225"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6i9E2-wTb5q"
      },
      "source": [
        "predict the sentiments of testing data tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hENomi9TZLa"
      },
      "source": [
        "y_pred_nb = nb.predict(X_test_dtm)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4StssESuTgAv"
      },
      "source": [
        "\n",
        "measure the accuracy of our model on the testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efpJWD41TdxI",
        "outputId": "3e345a2b-e6d1-409b-8be9-760e9aa19ab1"
      },
      "source": [
        "from sklearn import metrics # import metrics from sklearn\n",
        "metrics.accuracy_score(y_test, y_pred_nb)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7717"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16A6qeVGUnIT"
      },
      "source": [
        "## **NAIVE BAYES ACCURACY - 0.7717**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtFtxJXaTmMJ"
      },
      "source": [
        "plot the confusion matrix between our predicted sentiments and the original testing data sentiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZ2lL_iDTmqc",
        "outputId": "d121be77-54bf-4ecb-d21a-2447def4cff5"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix # import confusion matrix from the sklearn.metrics\n",
        "confusion_matrix(y_test, y_pred_nb)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4137,  832],\n",
              "       [1451, 3580]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbsTJiZMTyp0"
      },
      "source": [
        "## **SVM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GSDvke6To7r"
      },
      "source": [
        "from sklearn.svm import LinearSVC # import SVC model from sklearn.svm\n",
        "svm_clf = LinearSVC(random_state=0) # get object of SVC model with random_state parameter = 0"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtYLmB2DT8XK",
        "outputId": "03621c4e-d1e9-4c13-c697-cd7eabe1566a"
      },
      "source": [
        "svm_clf.fit(X_train_dtm, y_train)# fit our both traing data tweets as well as its sentiments to the SVC model"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
              "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
              "          multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
              "          verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrMjK4gMT95i",
        "outputId": "649f58d3-1ed1-490c-9757-77af98a38da1"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score  # import cross_val_score from sklear.model_selection\n",
        "accuracies = cross_val_score(estimator = svm_clf, X = X_train_dtm, y = y_train, cv = 10)# do K- fold cross validation on our traing data and its sentimenst with 10 fold cross validation\n",
        "accuracies.mean() # measure the mean accuray of 10 fold cross validation"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7872"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8sgg5dsT_0e"
      },
      "source": [
        "y_pred_svm = svm_clf.predict(X_test_dtm)  # predict the sentiments of testing data tweets"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdgwmddaUMK4",
        "outputId": "3bf7e448-be5a-46c2-9ba7-856a2ff7defc"
      },
      "source": [
        "from sklearn import metrics  # import metrics from sklearn\n",
        "metrics.accuracy_score(y_test, y_pred_svm)  # measure the accuracy of our model on the testing data"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7899"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x6EI4oNUZ5t"
      },
      "source": [
        "# SVM ACCURACY - 0.7899"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Z9EvtFVUOGm",
        "outputId": "f8a5091b-a171-4739-df93-f7b1430f1a68"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix # import confusion matrix from the sklearn.metrics\n",
        "confusion_matrix(y_test, y_pred_svm)# plot the confusion matrix between our predicted sentiments and the original testing data sentiments"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4078,  891],\n",
              "       [1210, 3821]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCHcH4GeUQc6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}